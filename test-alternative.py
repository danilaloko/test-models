import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
import sys

def load_alternative_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –≤ —Å–ª—É—á–∞–µ –ø—Ä–æ–±–ª–µ–º —Å NSFW-3B"""
    try:
        # –°–ø–∏—Å–æ–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        models_to_try = [
            "microsoft/DialoGPT-medium",
            "microsoft/DialoGPT-small", 
            "gpt2",
            "distilgpt2"
        ]
        
        for model_name in models_to_try:
            try:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {model_name}")
                
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                )
                
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
                if torch.cuda.is_available():
                    model = model.to("cuda")
                
                print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                return model, tokenizer, model_name
                
            except Exception as e:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {model_name}: {e}")
                continue
        
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –∏–∑ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        return None, None, None
        
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return None, None, None

def generate_response_alternative(model, tokenizer, user_input, model_name):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –¥–ª—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    try:
        # –î–ª—è DialoGPT –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        if "DialoGPT" in model_name:
            # –ö–æ–¥–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors="pt")
            
            if torch.cuda.is_available():
                inputs = inputs.to(model.device)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_new_tokens=100,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
            return response.strip()
        
        else:
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö GPT –º–æ–¥–µ–ª–µ–π
            inputs = tokenizer(user_input, return_tensors="pt", truncation=True, max_length=512)
            
            if torch.cuda.is_available():
                inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            return response.strip()
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
    model, tokenizer, model_name = load_alternative_model()
    
    if model is None:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        return
    
    print(f"\n{'='*60}")
    print(f"ü§ñ –ß–∞—Ç —Å {model_name} –∑–∞–ø—É—â–µ–Ω!")
    print("–í–≤–µ–¥–∏—Ç–µ 'quit', 'exit' –∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è")
    print(f"{'='*60}\n")
    
    while True:
        try:
            user_input = input("\nüë§ –í—ã: ").strip()
            
            if user_input.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥', 'q']:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            if not user_input:
                continue
            
            print("\nü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: ", end="", flush=True)
            response = generate_response_alternative(model, tokenizer, user_input, model_name)
            print(response)
            
        except KeyboardInterrupt:
            print("\n\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
            continue

if __name__ == "__main__":
    main()
