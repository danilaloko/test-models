import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
import sys

def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    try:
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å NSFW-3B...")
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–∫–∏ —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏
        load_configs = [
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 1: –ë–µ–∑ device_map, CPU —Å–Ω–∞—á–∞–ª–∞
            {
                "trust_remote_code": True,
                "torch_dtype": torch.float32,
                "low_cpu_mem_usage": True,
                "use_safetensors": False
            },
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 2: float16 –±–µ–∑ device_map
            {
                "trust_remote_code": True,
                "torch_dtype": torch.float16,
                "low_cpu_mem_usage": True,
                "use_safetensors": False
            },
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 3: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            {
                "trust_remote_code": True,
                "torch_dtype": torch.float16,
                "device_map": "auto",
                "low_cpu_mem_usage": True,
                "use_safetensors": False,
                "attn_implementation": "eager"  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º eager attention
            }
        ]
        
        model = None
        for i, config in enumerate(load_configs):
            try:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {i+1}/3...")
                model = AutoModelForCausalLM.from_pretrained(
                    "UnfilteredAI/NSFW-3B",
                    **config
                )
                
                # –ï—Å–ª–∏ device_map –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ CUDA –≤—Ä—É—á–Ω—É—é
                if "device_map" not in config and torch.cuda.is_available():
                    model = model.to("cuda")
                
                print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è {i+1})")
                break
                
            except Exception as e:
                print(f"‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è {i+1} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {e}")
                if i == len(load_configs) - 1:
                    print("–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å")
                    return None, None
                continue
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        tokenizer = AutoTokenizer.from_pretrained(
            "UnfilteredAI/NSFW-3B", 
            trust_remote_code=True
        )
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        try:
            print("–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å...")
            test_input = tokenizer("–¢–µ—Å—Ç", return_tensors="pt")
            if torch.cuda.is_available() and model.device.type == 'cuda':
                test_input = {k: v.to(model.device) for k, v in test_input.items()}
            
            with torch.no_grad():
                _ = model(**test_input)
            print("‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ —Ç–µ—Å—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏")
            
        except Exception as e:
            print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—à–ª–∞ —Ç–µ—Å—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: {e}")
            return None, None
            
        return model, tokenizer
        
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        return None, None

def create_prompt(system_message, user_message):
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    return f"""<|im_start|>system
{system_message}
<|im_end|>
<|im_start|>user
{user_message}
<|im_end|>
<|im_start|>assistant
"""

def generate_response(model, tokenizer, prompt, max_length=512):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã
        inputs = tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=1024,  # –£–º–µ–Ω—å—à–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
            padding=False
        )
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        if torch.cuda.is_available() and model.device.type == 'cuda':
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        print(f"–†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {inputs['input_ids'].shape}")
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–∏–º–µ—Ä –¥–ª—è –≤—ã–≤–æ–¥–∞ (–Ω–æ –±–µ–∑ –Ω–µ–≥–æ —Å–Ω–∞—á–∞–ª–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
        # streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # –ü–æ–ø—ã—Ç–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        generation_configs = [
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 1: –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è
            {
                "max_new_tokens": min(max_length, 256),
                "temperature": 0.7,
                "do_sample": True,
                "top_p": 0.9,
                "pad_token_id": tokenizer.eos_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "use_cache": False,  # –û—Ç–∫–ª—é—á–∞–µ–º –∫–µ—à –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º
                "repetition_penalty": 1.1
            },
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 2: –ñ–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫
            {
                "max_new_tokens": min(max_length, 128),
                "do_sample": False,  # –ñ–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫
                "pad_token_id": tokenizer.eos_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "use_cache": False
            },
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 3: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è
            {
                "max_new_tokens": 50,
                "temperature": 1.0,
                "do_sample": True,
                "pad_token_id": tokenizer.eos_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "use_cache": False
            }
        ]
        
        for i, config in enumerate(generation_configs):
            try:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ {i+1}/3 —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: max_new_tokens={config['max_new_tokens']}")
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        **config
                    )
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]
                response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                
                if response.strip():  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ –ø—É—Å—Ç–æ–π
                    print(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞ (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è {i+1})")
                    return response.strip()
                else:
                    print(f"‚ö†Ô∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è {i+1} –¥–∞–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                    continue
                    
            except Exception as e:
                print(f"‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ {i+1} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {e}")
                if i == len(generation_configs) - 1:
                    return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫."
                continue
        
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å –Ω–µ—É–¥–∞—á–µ–π."
        
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—â–µ–Ω–∏—è —Å –º–æ–¥–µ–ª—å—é"""
    model, tokenizer = load_model()
    
    if model is None or tokenizer is None:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        return
    
    print("\n" + "="*50)
    print("ü§ñ –ß–∞—Ç —Å NSFW-3B –∑–∞–ø—É—â–µ–Ω!")
    print("–í–≤–µ–¥–∏—Ç–µ 'quit', 'exit' –∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è")
    print("–í–≤–µ–¥–∏—Ç–µ 'clear' –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏")
    print("="*50 + "\n")
    
    # –°–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    system_message = "You are a helpful AI assistant."
    
    while True:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_input = input("\nüë§ –í—ã: ").strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–∞–Ω–¥—ã –≤—ã—Ö–æ–¥–∞
            if user_input.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥', 'q']:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–∞–Ω–¥—É –æ—á–∏—Å—Ç–∫–∏
            if user_input.lower() in ['clear', '–æ—á–∏—Å—Ç–∏—Ç—å']:
                print("üßπ –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞!")
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            if not user_input:
                continue
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = create_prompt(system_message, user_input)
            
            print("\nü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: ", end="", flush=True)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = generate_response(model, tokenizer, prompt)
            
            print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞
            
        except KeyboardInterrupt:
            print("\n\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
            continue

if __name__ == "__main__":
    main()
