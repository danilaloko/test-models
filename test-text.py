import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
import sys

def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    try:
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å NSFW-3B...")
        model = AutoModelForCausalLM.from_pretrained(
            "UnfilteredAI/NSFW-3B", 
            trust_remote_code=True, 
            torch_dtype=torch.float16,  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å bfloat16 –Ω–∞ float16
            device_map="auto"  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –Ω–∞ GPU
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            "UnfilteredAI/NSFW-3B", 
            trust_remote_code=True
        )
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        return model, tokenizer
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        return None, None

def create_prompt(system_message, user_message):
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    return f"""<|im_start|>system
{system_message}
<|im_end|>
<|im_start|>user
{user_message}
<|im_end|>
<|im_start|>assistant
"""

def generate_response(model, tokenizer, prompt, max_length=512):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏"""
    try:
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        if torch.cuda.is_available():
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–∏–º–µ—Ä –¥–ª—è –≤—ã–≤–æ–¥–∞
        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                streamer=streamer,
                use_cache=True
            )
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.strip()
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—â–µ–Ω–∏—è —Å –º–æ–¥–µ–ª—å—é"""
    model, tokenizer = load_model()
    
    if model is None or tokenizer is None:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        return
    
    print("\n" + "="*50)
    print("ü§ñ –ß–∞—Ç —Å NSFW-3B –∑–∞–ø—É—â–µ–Ω!")
    print("–í–≤–µ–¥–∏—Ç–µ 'quit', 'exit' –∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è")
    print("–í–≤–µ–¥–∏—Ç–µ 'clear' –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏")
    print("="*50 + "\n")
    
    # –°–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    system_message = "You are a helpful AI assistant."
    
    while True:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_input = input("\nüë§ –í—ã: ").strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–∞–Ω–¥—ã –≤—ã—Ö–æ–¥–∞
            if user_input.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥', 'q']:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–∞–Ω–¥—É –æ—á–∏—Å—Ç–∫–∏
            if user_input.lower() in ['clear', '–æ—á–∏—Å—Ç–∏—Ç—å']:
                print("üßπ –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞!")
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            if not user_input:
                continue
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = create_prompt(system_message, user_input)
            
            print("\nü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: ", end="", flush=True)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = generate_response(model, tokenizer, prompt)
            
            print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞
            
        except KeyboardInterrupt:
            print("\n\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
            continue

if __name__ == "__main__":
    main()
