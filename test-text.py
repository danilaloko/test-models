import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
import sys

def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    try:
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å NSFW-3B...")
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏
        load_options = [
            {
                "torch_dtype": torch.float32,
                "device_map": None,
                "low_cpu_mem_usage": True
            },
            {
                "torch_dtype": torch.float16,
                "device_map": "auto",
                "low_cpu_mem_usage": True
            },
            {
                "torch_dtype": torch.float32,
                "device_map": "auto",
                "low_cpu_mem_usage": True
            }
        ]
        
        model = None
        for i, options in enumerate(load_options):
            try:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {i+1}/3 —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {options}")
                model = AutoModelForCausalLM.from_pretrained(
                    "UnfilteredAI/NSFW-3B", 
                    trust_remote_code=True,
                    **options
                )
                
                # –ï—Å–ª–∏ device_map –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –ø–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Ä—É—á–Ω—É—é
                if options["device_map"] is None and torch.cuda.is_available():
                    model = model.to("cuda")
                
                print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                break
                
            except Exception as e:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {i+1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if i == len(load_options) - 1:
                    raise e
                continue
        
        if model is None:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∏ –æ–¥–Ω–∏–º –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        tokenizer = AutoTokenizer.from_pretrained(
            "UnfilteredAI/NSFW-3B", 
            trust_remote_code=True
        )
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        return model, tokenizer
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        return None, None

def create_prompt(system_message, user_message):
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    return f"""<|im_start|>system
{system_message}
<|im_end|>
<|im_start|>user
{user_message}
<|im_end|>
<|im_start|>assistant
"""

def generate_response(model, tokenizer, prompt, max_length=512):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏"""
    try:
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç —Å –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        inputs = tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=1024,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 2048
            padding=True
        )
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        if torch.cuda.is_available():
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–∏–º–µ—Ä –¥–ª—è –≤—ã–≤–æ–¥–∞
        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        generation_options = [
            {
                "max_new_tokens": max_length,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "pad_token_id": tokenizer.eos_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "streamer": streamer,
                "use_cache": True,
                "repetition_penalty": 1.1
            },
            {
                "max_new_tokens": max_length,
                "temperature": 0.8,
                "top_p": 0.95,
                "do_sample": True,
                "pad_token_id": tokenizer.eos_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "use_cache": False,  # –û—Ç–∫–ª—é—á–∞–µ–º –∫–µ—à
                "repetition_penalty": 1.1
            },
            {
                "max_new_tokens": max_length,
                "temperature": 0.7,
                "do_sample": False,  # –ñ–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫
                "pad_token_id": tokenizer.eos_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "use_cache": False
            }
        ]
        
        for i, options in enumerate(generation_options):
            try:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ {i+1}/3...")
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        **options
                    )
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                response = tokenizer.decode(
                    outputs[0][inputs['input_ids'].shape[1]:], 
                    skip_special_tokens=True
                )
                return response.strip()
                
            except Exception as e:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ {i+1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if i == len(generation_options) - 1:
                    raise e
                continue
        
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—â–µ–Ω–∏—è —Å –º–æ–¥–µ–ª—å—é"""
    model, tokenizer = load_model()
    
    if model is None or tokenizer is None:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        return
    
    print("\n" + "="*50)
    print("ü§ñ –ß–∞—Ç —Å NSFW-3B –∑–∞–ø—É—â–µ–Ω!")
    print("–í–≤–µ–¥–∏—Ç–µ 'quit', 'exit' –∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è")
    print("–í–≤–µ–¥–∏—Ç–µ 'clear' –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏")
    print("="*50 + "\n")
    
    # –°–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    system_message = "You are a helpful AI assistant."
    
    while True:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_input = input("\nüë§ –í—ã: ").strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–∞–Ω–¥—ã –≤—ã—Ö–æ–¥–∞
            if user_input.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥', 'q']:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–∞–Ω–¥—É –æ—á–∏—Å—Ç–∫–∏
            if user_input.lower() in ['clear', '–æ—á–∏—Å—Ç–∏—Ç—å']:
                print("üßπ –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞!")
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            if not user_input:
                continue
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = create_prompt(system_message, user_input)
            
            print("\nü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: ", end="", flush=True)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = generate_response(model, tokenizer, prompt)
            
            print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞
            
        except KeyboardInterrupt:
            print("\n\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
            continue

if __name__ == "__main__":
    main()
