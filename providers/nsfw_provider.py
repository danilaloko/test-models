"""
–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω—ã—Ö NSFW –º–æ–¥–µ–ª–µ–π
"""

import os
import torch
import asyncio
from typing import Dict, Any, Optional
from dotenv import load_dotenv
from diffusers import DiffusionPipeline

from .base_provider import BaseProvider

class NSFWProvider(BaseProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ NSFW –º–æ–¥–µ–ª—è–º–∏"""
    
    def __init__(self):
        super().__init__(
            name="NSFW Local",
            description="–õ–æ–∫–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é NSFW –º–æ–¥–µ–ª–µ–π (—Ç—Ä–µ–±—É–µ—Ç GPU)"
        )
        self.pipeline: Optional[DiffusionPipeline] = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.available_models = [
            "UnfilteredAI/NSFW-gen-v2",
            "UnfilteredAI/NSFW-3B"
        ]
        self.current_model = self.available_models[0]
    
    async def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        try:
            load_dotenv()
            hf_token = os.getenv("HF_TOKEN")
            
            if not hf_token:
                print("‚ùå HF_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
                return False
            
            if not torch.cuda.is_available():
                print("‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU (–º–µ–¥–ª–µ–Ω–Ω–æ)")
                self.device = "cpu"
            
            print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å {self.current_model}...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            self.pipeline = await self._run_sync_in_async(
                self._load_pipeline,
                model_id=self.current_model,
                hf_token=hf_token
            )
            
            if self.pipeline is None:
                return False
            
            self.is_initialized = True
            print(f"‚úÖ {self.name} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ –Ω–∞ {self.device}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ {self.name}: {e}")
            return False
    
    def _load_pipeline(self, model_id: str, hf_token: str) -> Optional[DiffusionPipeline]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏
            load_configs = [
                {
                    "torch_dtype": torch.float16,
                    "use_auth_token": hf_token,
                    "low_cpu_mem_usage": True
                },
                {
                    "torch_dtype": torch.bfloat16,
                    "use_auth_token": hf_token,
                    "low_cpu_mem_usage": True
                },
                {
                    "torch_dtype": torch.float32,
                    "use_auth_token": hf_token,
                    "low_cpu_mem_usage": True
                }
            ]
            
            for i, config in enumerate(load_configs):
                try:
                    print(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {i+1}/{len(load_configs)}...")
                    pipeline = DiffusionPipeline.from_pretrained(model_id, **config)
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                    if self.device == "cuda":
                        pipeline = pipeline.to("cuda")
                    
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è {i+1})")
                    return pipeline
                    
                except Exception as e:
                    print(f"‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è {i+1} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {e}")
                    continue
            
            print("‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å")
            return None
            
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return None
    
    async def generate_image(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é NSFW –º–æ–¥–µ–ª–∏
        
        Args:
            prompt: –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
                - width: int - —à–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 512)
                - height: int - –≤—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 512)
                - num_inference_steps: int - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20)
                - guidance_scale: float - —Å–∏–ª–∞ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 7.5)
                - num_images: int - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1)
        """
        if not self.is_available():
            return {
                "success": False,
                "error": "–ü—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
            }
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            width = kwargs.get("width", 512)
            height = kwargs.get("height", 512)
            num_inference_steps = kwargs.get("num_inference_steps", 20)
            guidance_scale = kwargs.get("guidance_scale", 7.5)
            num_images = kwargs.get("num_images", 1)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
            image_path = self._generate_filename(prompt)
            
            print(f"üé® –ì–µ–Ω–µ—Ä–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ {self.current_model}...")
            print(f"üìù –ü—Ä–æ–º–ø—Ç: {prompt}")
            print(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            result = await self._run_sync_in_async(
                self._generate_image_sync,
                prompt=prompt,
                width=width,
                height=height,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                num_images=num_images
            )
            
            if result is None:
                return {
                    "success": False,
                    "error": "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
                }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            result.images[0].save(image_path)
            
            print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {image_path}")
            
            return {
                "success": True,
                "image_path": image_path,
                "metadata": {
                    "model": self.current_model,
                    "prompt": prompt,
                    "width": width,
                    "height": height,
                    "num_inference_steps": num_inference_steps,
                    "guidance_scale": guidance_scale,
                    "num_images": num_images,
                    "device": self.device,
                    "provider": self.name
                }
            }
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ {self.name}: {str(e)}"
            print(f"‚ùå {error_msg}")
            return {
                "success": False,
                "error": error_msg
            }
    
    def _generate_image_sync(self, prompt: str, width: int, height: int, 
                           num_inference_steps: int, guidance_scale: float, num_images: int):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            with torch.no_grad():
                result = self.pipeline(
                    prompt=prompt,
                    width=width,
                    height=height,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    num_images_per_prompt=num_images
                )
            return result
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return None
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        return self.is_initialized and self.pipeline is not None
    
    def set_model(self, model: str) -> bool:
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            
        Returns:
            True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ
        """
        if model in self.available_models:
            self.current_model = model
            # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
            self.is_initialized = False
            self.pipeline = None
            return True
        return False
    
    def get_available_models(self) -> list:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return self.available_models.copy()
    
    def get_current_model(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å"""
        return self.current_model
    
    def get_device(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        return self.device
